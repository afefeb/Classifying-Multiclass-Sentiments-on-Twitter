{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer,TFBertForSequenceClassification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>label_angka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>kunjungan prabowo untuk meresmikan menyerahkan...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "      <td>kunjung prabowo untuk resmi serah proyek bantu...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>anies tepuk tangan meriah jadi rektor mewajibk...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>anies tepuk tangan riah jadi rektor wajib mata...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>emng bener sih pendukung anies juga pendukung ...</td>\n",
       "      <td>Demografi</td>\n",
       "      <td>emng bener sih dukung anies juga dukung prabow...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sewaktu anies bersikap kritis kinerja pak prab...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>waktu anies sikap kritis kerja pak prabowo ang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>anies baswedan harap asn termasuk tni polri pe...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>anies baswedan harap asn masuk tni polri pegan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4578</th>\n",
       "      <td>4578</td>\n",
       "      <td>ngeliat debat kemaren pas prabowo kicep kekira...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>ngeliat debat kemaren pas prabowo kicep kira k...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4579</th>\n",
       "      <td>4579</td>\n",
       "      <td>masyarakat yakin prabowo gibran memiliki visi ...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>masyarakat yakin prabowo gibran milik visi jal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4580</th>\n",
       "      <td>4580</td>\n",
       "      <td>imo both are irrational but satu jauh lebih ir...</td>\n",
       "      <td>Ekonomi</td>\n",
       "      <td>imo both are irrational but satu jauh lebih ir...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4581</th>\n",
       "      <td>4581</td>\n",
       "      <td>look at that pak ganjar sudah berkecimpung lgi...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>look at that pak ganjar sudah kecimpung lgisla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4582</th>\n",
       "      <td>4582</td>\n",
       "      <td>acara tidak memasak calon presiden nomor urut ...</td>\n",
       "      <td>Sumber Daya Alam</td>\n",
       "      <td>acara tidak masak calon presiden nomor urut 2 ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4583 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  kunjungan prabowo untuk meresmikan menyerahkan...   \n",
       "1              1  anies tepuk tangan meriah jadi rektor mewajibk...   \n",
       "2              2  emng bener sih pendukung anies juga pendukung ...   \n",
       "3              3  sewaktu anies bersikap kritis kinerja pak prab...   \n",
       "4              4  anies baswedan harap asn termasuk tni polri pe...   \n",
       "...          ...                                                ...   \n",
       "4578        4578  ngeliat debat kemaren pas prabowo kicep kekira...   \n",
       "4579        4579  masyarakat yakin prabowo gibran memiliki visi ...   \n",
       "4580        4580  imo both are irrational but satu jauh lebih ir...   \n",
       "4581        4581  look at that pak ganjar sudah berkecimpung lgi...   \n",
       "4582        4582  acara tidak memasak calon presiden nomor urut ...   \n",
       "\n",
       "                        label  \\\n",
       "0            Sumber Daya Alam   \n",
       "1                     Politik   \n",
       "2                   Demografi   \n",
       "3                     Politik   \n",
       "4                     Politik   \n",
       "...                       ...   \n",
       "4578                  Politik   \n",
       "4579                  Politik   \n",
       "4580                  Ekonomi   \n",
       "4581  Pertahanan dan Keamanan   \n",
       "4582         Sumber Daya Alam   \n",
       "\n",
       "                                           stemmed_text  label_angka  \n",
       "0     kunjung prabowo untuk resmi serah proyek bantu...            5  \n",
       "1     anies tepuk tangan riah jadi rektor wajib mata...            0  \n",
       "2     emng bener sih dukung anies juga dukung prabow...            6  \n",
       "3     waktu anies sikap kritis kerja pak prabowo ang...            0  \n",
       "4     anies baswedan harap asn masuk tni polri pegan...            0  \n",
       "...                                                 ...          ...  \n",
       "4578  ngeliat debat kemaren pas prabowo kicep kira k...            0  \n",
       "4579  masyarakat yakin prabowo gibran milik visi jal...            0  \n",
       "4580  imo both are irrational but satu jauh lebih ir...            4  \n",
       "4581  look at that pak ganjar sudah kecimpung lgisla...            3  \n",
       "4582  acara tidak masak calon presiden nomor urut 2 ...            5  \n",
       "\n",
       "[4583 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\thebl\\\\Documents\\\\Lomba\\\\SatDat2024\\\\FullSenyum-SatriaData2024\\\\DataSet\\\\cleaned_final_label.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stemmed_text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>label_angka</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>sewaktu anies bersikap kritis kinerja pak prab...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>waktu anies sikap kritis kerja pak prabowo ang...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>bulan januari serangan kubu dkk semakin sengit...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>bulan januari serang kubu dkk makin sengit mas...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>141</td>\n",
       "      <td>bulan januari serangan kubu dkk semakin sengit...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>bulan januari serang kubu dkk makin sengit mas...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>bulan januari serangan kubu dkk semakin sengit...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>bulan januari serang kubu dkk makin sengit mas...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>capres cawapres ganjar pranowo mahfud md menga...</td>\n",
       "      <td>Ideologi</td>\n",
       "      <td>capres cawapres ganjar pranowo mahfud md ajak ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520</th>\n",
       "      <td>4520</td>\n",
       "      <td>netizen geger haikal hassan gunakan politik id...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>netizen geger haikal hassan guna politik ident...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>4522</td>\n",
       "      <td>cukup lihat track recordnya ganjar pranowo mah...</td>\n",
       "      <td>Politik</td>\n",
       "      <td>cukup lihat track recordnya ganjar pranowo mah...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4544</th>\n",
       "      <td>4544</td>\n",
       "      <td>bukan cuma tampilan luarnya kebijakan ganjar p...</td>\n",
       "      <td>Ideologi</td>\n",
       "      <td>bukan cuma tampil luar bijak ganjar pranowo ma...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4574</th>\n",
       "      <td>4574</td>\n",
       "      <td>bulan januari serangan kubu dkk semakin sengit...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>bulan januari serang kubu dkk makin sengit mas...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4577</th>\n",
       "      <td>4577</td>\n",
       "      <td>tanggapan serius ganjar atas tindakan pengania...</td>\n",
       "      <td>Pertahanan dan Keamanan</td>\n",
       "      <td>tanggap serius ganjar atas tindak aniaya timpa...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "59            59  sewaktu anies bersikap kritis kinerja pak prab...   \n",
       "126          126  bulan januari serangan kubu dkk semakin sengit...   \n",
       "141          141  bulan januari serangan kubu dkk semakin sengit...   \n",
       "147          147  bulan januari serangan kubu dkk semakin sengit...   \n",
       "148          148  capres cawapres ganjar pranowo mahfud md menga...   \n",
       "...          ...                                                ...   \n",
       "4520        4520  netizen geger haikal hassan gunakan politik id...   \n",
       "4522        4522  cukup lihat track recordnya ganjar pranowo mah...   \n",
       "4544        4544  bukan cuma tampilan luarnya kebijakan ganjar p...   \n",
       "4574        4574  bulan januari serangan kubu dkk semakin sengit...   \n",
       "4577        4577  tanggapan serius ganjar atas tindakan pengania...   \n",
       "\n",
       "                        label  \\\n",
       "59                    Politik   \n",
       "126   Pertahanan dan Keamanan   \n",
       "141   Pertahanan dan Keamanan   \n",
       "147   Pertahanan dan Keamanan   \n",
       "148                  Ideologi   \n",
       "...                       ...   \n",
       "4520                  Politik   \n",
       "4522                  Politik   \n",
       "4544                 Ideologi   \n",
       "4574  Pertahanan dan Keamanan   \n",
       "4577  Pertahanan dan Keamanan   \n",
       "\n",
       "                                           stemmed_text  label_angka  \n",
       "59    waktu anies sikap kritis kerja pak prabowo ang...            0  \n",
       "126   bulan januari serang kubu dkk makin sengit mas...            3  \n",
       "141   bulan januari serang kubu dkk makin sengit mas...            3  \n",
       "147   bulan januari serang kubu dkk makin sengit mas...            3  \n",
       "148   capres cawapres ganjar pranowo mahfud md ajak ...            2  \n",
       "...                                                 ...          ...  \n",
       "4520  netizen geger haikal hassan guna politik ident...            0  \n",
       "4522  cukup lihat track recordnya ganjar pranowo mah...            0  \n",
       "4544  bukan cuma tampil luar bijak ganjar pranowo ma...            2  \n",
       "4574  bulan januari serang kubu dkk makin sengit mas...            3  \n",
       "4577  tanggap serius ganjar atas tindak aniaya timpa...            3  \n",
       "\n",
       "[312 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['stemmed_text'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop_duplicates(subset='stemmed_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4271, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = df['stemmed_text'].apply(tokenize_function).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.array([x['input_ids'].numpy() for x in tokenized_datasets])\n",
    "attention_mask = np.array([x['attention_mask'].numpy() for x in tokenized_datasets])\n",
    "labels = df['label_angka'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "input_ids_resampled, labels_resampled = smote.fit_resample(input_ids.reshape(-1, input_ids.shape[-1]), labels)\n",
    "attention_mask_resampled, _ = smote.fit_resample(attention_mask.reshape(-1, attention_mask.shape[-1]), labels)\n",
    "\n",
    "input_ids_resampled = input_ids_resampled.reshape(-1, input_ids.shape[-1])\n",
    "attention_mask_resampled = attention_mask_resampled.reshape(-1, attention_mask.shape[-1])\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_resampled = to_categorical(labels_resampled, num_classes=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(input_ids_resampled, labels_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_mask_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: X_train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mattention_mask_train\u001b[49m}, y_train))\n\u001b[0;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      4\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: X_val, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask_val}, y_val))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'attention_mask_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "eval_dataset = eval_dataset.batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "# Initialize BERT model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy('accuracy')]\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = bert_model.fit(train_dataset, epochs=3, validation_data=eval_dataset)\n",
    "eval_results = bert_model.evaluate(eval_dataset)\n",
    "print(f'Evaluation results: {eval_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute '_distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mCategoricalAccuracy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     46\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39meval_dataset)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:1563\u001b[0m, in \u001b[0;36mTFPreTrainedModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps_per_execution\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[1;32m-> 1563\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1574\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1575\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m   1576\u001b[0m         loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4021\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended.variable_created_in_scope\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m   4020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvariable_created_in_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[1;32m-> 4021\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribute_strategy\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Variable' object has no attribute '_distribute_strategy'"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = df['stemmed_text'].apply(tokenize_function).tolist()\n",
    "\n",
    "# Extract input_ids, attention_mask, and labels\n",
    "input_ids = np.array([x['input_ids'].numpy()[0] for x in tokenized_datasets])\n",
    "attention_mask = np.array([x['attention_mask'].numpy()[0] for x in tokenized_datasets])\n",
    "labels = df['label_angka'].values\n",
    "\n",
    "# Apply SMOTE on tokenized inputs\n",
    "smote = SMOTE(random_state=42)\n",
    "input_ids_resampled, labels_resampled = smote.fit_resample(input_ids, labels)\n",
    "attention_mask_resampled, _ = smote.fit_resample(attention_mask, labels)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_resampled = to_categorical(labels_resampled, num_classes=8)\n",
    "\n",
    "# Split the resampled data into training and evaluation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids_resampled, labels_resampled, test_size=0.2, random_state=42)\n",
    "attention_mask_train, attention_mask_val = train_test_split(attention_mask_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "eval_dataset = eval_dataset.batch(16)\n",
    "\n",
    "# Initialize model\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Compile the model with optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=3, validation_data=eval_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.evaluate(eval_dataset)\n",
    "print(f'Evaluation results: {eval_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Loss: 2.124677896499634\n",
      "Loss: 2.23899507522583\n",
      "Loss: 2.064851760864258\n",
      "Loss: 2.0310683250427246\n",
      "Loss: 2.0390005111694336\n",
      "Loss: 2.0471906661987305\n",
      "Loss: 2.0214178562164307\n",
      "Loss: 2.1140971183776855\n",
      "Loss: 2.1171064376831055\n",
      "Loss: 2.0170905590057373\n",
      "Loss: 2.038987636566162\n",
      "Loss: 2.1253232955932617\n",
      "Loss: 2.038936138153076\n",
      "Loss: 2.0582666397094727\n",
      "Loss: 2.0780587196350098\n",
      "Loss: 2.038452386856079\n",
      "Loss: 2.0860068798065186\n",
      "Loss: 1.9561851024627686\n",
      "Loss: 2.0551905632019043\n",
      "Loss: 1.9892600774765015\n",
      "Loss: 2.0119781494140625\n",
      "Loss: 2.0600168704986572\n",
      "Loss: 2.0379233360290527\n",
      "Loss: 2.0168566703796387\n",
      "Loss: 2.0112318992614746\n",
      "Loss: 1.9914896488189697\n",
      "Loss: 2.023820400238037\n",
      "Loss: 2.0916481018066406\n",
      "Loss: 2.0121521949768066\n",
      "Loss: 2.020667552947998\n",
      "Loss: 1.9617750644683838\n",
      "Loss: 2.0965704917907715\n",
      "Loss: 1.9668402671813965\n",
      "Loss: 2.004055976867676\n",
      "Loss: 1.9880656003952026\n",
      "Loss: 1.88955557346344\n",
      "Loss: 1.9483811855316162\n",
      "Loss: 2.010159492492676\n",
      "Loss: 2.0598039627075195\n",
      "Loss: 1.6285480260849\n",
      "Loss: 1.9564285278320312\n",
      "Loss: 1.9236059188842773\n",
      "Loss: 1.8804466724395752\n",
      "Loss: 2.0696964263916016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     72\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# Ensure df has 'stemmed_text' and 'label_angka' columns\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = df['stemmed_text'].apply(tokenize_function).tolist()\n",
    "\n",
    "# Extract input_ids, attention_mask, and labels\n",
    "input_ids = np.array([x['input_ids'][0] for x in tokenized_datasets])\n",
    "attention_mask = np.array([x['attention_mask'][0] for x in tokenized_datasets])\n",
    "labels = df['label_angka'].values\n",
    "\n",
    "# Apply SMOTE on tokenized inputs\n",
    "smote = SMOTE(random_state=42)\n",
    "input_ids_resampled, labels_resampled = smote.fit_resample(input_ids, labels)\n",
    "attention_mask_resampled, _ = smote.fit_resample(attention_mask, labels)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_resampled = to_categorical(labels_resampled, num_classes=8)\n",
    "\n",
    "# Split the resampled data into training and evaluation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids_resampled, labels_resampled, test_size=0.2, random_state=42)\n",
    "attention_mask_train, attention_mask_val = train_test_split(attention_mask_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "eval_dataset = eval_dataset.batch(16)\n",
    "\n",
    "# Initialize model\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training loop using tf.GradientTape\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_mask, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=True)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch+1}/{3}')\n",
    "    for batch in train_dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        loss = train_step(input_ids, attention_mask, labels)\n",
    "        print(f'Loss: {loss.numpy()}')\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(dataset):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for batch in dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=False)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "        total_loss += loss.numpy()\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(labels, axis=-1)), tf.float32))\n",
    "        total_accuracy += accuracy.numpy()\n",
    "        num_batches += 1\n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "eval_loss, eval_accuracy = evaluate(eval_dataset)\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "print(f'Evaluation Accuracy: {eval_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Loss: 2.129279136657715\n",
      "Loss: 2.116427421569824\n",
      "Loss: 2.1419947147369385\n",
      "Loss: 2.0599160194396973\n",
      "Loss: 2.217255115509033\n",
      "Loss: 2.0580785274505615\n",
      "Loss: 2.116018772125244\n",
      "Loss: 2.0919649600982666\n",
      "Loss: 2.0816457271575928\n",
      "Loss: 2.1161999702453613\n",
      "Loss: 2.081799268722534\n",
      "Loss: 2.1426610946655273\n",
      "Loss: 2.08425235748291\n",
      "Loss: 2.0863564014434814\n",
      "Loss: 2.1414875984191895\n",
      "Loss: 2.1100540161132812\n",
      "Loss: 2.1504645347595215\n",
      "Loss: 2.032630681991577\n",
      "Loss: 2.101759672164917\n",
      "Loss: 2.0692126750946045\n",
      "Loss: 2.1306400299072266\n",
      "Loss: 2.1066277027130127\n",
      "Loss: 2.185425281524658\n",
      "Loss: 2.1067886352539062\n",
      "Loss: 2.0948374271392822\n",
      "Loss: 2.0500545501708984\n",
      "Loss: 2.0723071098327637\n",
      "Loss: 2.1084060668945312\n",
      "Loss: 2.0574021339416504\n",
      "Loss: 2.151772975921631\n",
      "Loss: 2.0436043739318848\n",
      "Loss: 2.139460563659668\n",
      "Loss: 2.0520994663238525\n",
      "Loss: 2.075068950653076\n",
      "Loss: 2.081045627593994\n",
      "Loss: 2.0998120307922363\n",
      "Loss: 2.0629658699035645\n",
      "Loss: 2.130089282989502\n",
      "Loss: 2.1184377670288086\n",
      "Loss: 2.099052906036377\n",
      "Loss: 2.0368833541870117\n",
      "Loss: 2.0742478370666504\n",
      "Loss: 2.0857810974121094\n",
      "Loss: 2.0382204055786133\n",
      "Loss: 2.0686450004577637\n",
      "Loss: 2.0619919300079346\n",
      "Loss: 2.0482778549194336\n",
      "Loss: 2.131235122680664\n",
      "Loss: 2.0718283653259277\n",
      "Loss: 2.1120240688323975\n",
      "Loss: 2.103529691696167\n",
      "Loss: 2.123744487762451\n",
      "Loss: 2.057281494140625\n",
      "Loss: 2.104569911956787\n",
      "Loss: 2.034709930419922\n",
      "Loss: 2.1011407375335693\n",
      "Loss: 2.0500168800354004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     69\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 70\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# Ensure df has 'stemmed_text' and 'label_angka' columns\n",
    "\n",
    "# Initialize tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Perform oversampling on the raw text data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "texts_resampled, labels_resampled = ros.fit_resample(df['stemmed_text'].values.reshape(-1, 1), df['label_angka'].values)\n",
    "texts_resampled = texts_resampled.flatten()\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = [tokenize_function(text) for text in texts_resampled]\n",
    "\n",
    "# Extract input_ids, attention_mask, and labels\n",
    "input_ids = np.array([x['input_ids'][0] for x in tokenized_datasets])\n",
    "attention_mask = np.array([x['attention_mask'][0] for x in tokenized_datasets])\n",
    "labels_resampled = to_categorical(labels_resampled, num_classes=8)\n",
    "\n",
    "# Split the resampled data into training and evaluation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels_resampled, test_size=0.2, random_state=42)\n",
    "attention_mask_train, attention_mask_val = train_test_split(attention_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "eval_dataset = eval_dataset.batch(16)\n",
    "\n",
    "# Initialize model\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training loop using tf.GradientTape\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_mask, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=True)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch+1}/{3}')\n",
    "    for batch in train_dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        loss = train_step(input_ids, attention_mask, labels)\n",
    "        print(f'Loss: {loss.numpy()}')\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(dataset):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for batch in dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=False)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "        total_loss += loss.numpy()\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(labels, axis=-1)), tf.float32))\n",
    "        total_accuracy += accuracy.numpy()\n",
    "        num_batches += 1\n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "eval_loss, eval_accuracy = evaluate(eval_dataset)\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "print(f'Evaluation Accuracy: {eval_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Loss: 2.0731682777404785\n",
      "Loss: 2.082730293273926\n",
      "Loss: 2.145451068878174\n",
      "Loss: 2.0821146965026855\n",
      "Loss: 2.081237316131592\n",
      "Loss: 2.0630857944488525\n",
      "Loss: 2.0679216384887695\n",
      "Loss: 2.137988328933716\n",
      "Loss: 2.054811954498291\n",
      "Loss: 2.0833520889282227\n",
      "Loss: 2.107038974761963\n",
      "Loss: 2.1097846031188965\n",
      "Loss: 2.0546927452087402\n",
      "Loss: 2.152414560317993\n",
      "Loss: 2.176971435546875\n",
      "Loss: 2.106440782546997\n",
      "Loss: 2.1655662059783936\n",
      "Loss: 2.0967016220092773\n",
      "Loss: 2.0924875736236572\n",
      "Loss: 2.057528495788574\n",
      "Loss: 2.098031997680664\n",
      "Loss: 2.1188442707061768\n",
      "Loss: 2.1649889945983887\n",
      "Loss: 2.094649314880371\n",
      "Loss: 2.0693411827087402\n",
      "Loss: 2.129472255706787\n",
      "Loss: 2.1079261302948\n",
      "Loss: 2.098616123199463\n",
      "Loss: 2.0925140380859375\n",
      "Loss: 2.102987766265869\n",
      "Loss: 2.077183723449707\n",
      "Loss: 2.1027705669403076\n",
      "Loss: 2.1246719360351562\n",
      "Loss: 2.0924949645996094\n",
      "Loss: 2.0994088649749756\n",
      "Loss: 2.1103172302246094\n",
      "Loss: 2.1360013484954834\n",
      "Loss: 2.12505841255188\n",
      "Loss: 2.1091299057006836\n",
      "Loss: 2.0911402702331543\n",
      "Loss: 2.0500717163085938\n",
      "Loss: 2.0607943534851074\n",
      "Loss: 2.0610909461975098\n",
      "Loss: 2.0611178874969482\n",
      "Loss: 2.0872445106506348\n",
      "Loss: 2.079864740371704\n",
      "Loss: 2.066723346710205\n",
      "Loss: 2.0816688537597656\n",
      "Loss: 2.0764880180358887\n",
      "Loss: 2.100541591644287\n",
      "Loss: 2.098583221435547\n",
      "Loss: 2.0619475841522217\n",
      "Loss: 2.102133274078369\n",
      "Loss: 2.04583740234375\n",
      "Loss: 2.0595598220825195\n",
      "Loss: 2.101879596710205\n",
      "Loss: 2.1070661544799805\n",
      "Loss: 2.1053829193115234\n",
      "Loss: 2.0315473079681396\n",
      "Loss: 2.060297727584839\n",
      "Loss: 2.078721523284912\n",
      "Loss: 2.089207649230957\n",
      "Loss: 2.107210874557495\n",
      "Loss: 2.0764431953430176\n",
      "Loss: 2.1127665042877197\n",
      "Loss: 2.007389783859253\n",
      "Loss: 2.102243661880493\n",
      "Loss: 2.0254812240600586\n",
      "Loss: 2.06764554977417\n",
      "Loss: 2.0803592205047607\n",
      "Loss: 2.040450096130371\n",
      "Loss: 2.052955389022827\n",
      "Loss: 2.081613779067993\n",
      "Loss: 2.056875228881836\n",
      "Loss: 2.0732014179229736\n",
      "Loss: 2.064619541168213\n",
      "Loss: 2.047321319580078\n",
      "Loss: 2.0701918601989746\n",
      "Loss: 2.0134801864624023\n",
      "Loss: 2.024951457977295\n",
      "Loss: 2.0460124015808105\n",
      "Loss: 2.059645175933838\n",
      "Loss: 2.1706252098083496\n",
      "Loss: 2.0096046924591064\n",
      "Loss: 2.0488760471343994\n",
      "Loss: 2.0532643795013428\n",
      "Loss: 1.958791971206665\n",
      "Loss: 2.0751914978027344\n",
      "Loss: 2.0395095348358154\n",
      "Loss: 2.0986275672912598\n",
      "Loss: 2.085068702697754\n",
      "Loss: 2.0710928440093994\n",
      "Loss: 2.119399070739746\n",
      "Loss: 2.070425510406494\n",
      "Loss: 2.008641004562378\n",
      "Loss: 2.089728832244873\n",
      "Loss: 2.071666717529297\n",
      "Loss: 1.9286305904388428\n",
      "Loss: 2.133977174758911\n",
      "Loss: 2.0181686878204346\n",
      "Loss: 2.019838333129883\n",
      "Loss: 1.9630924463272095\n",
      "Loss: 1.9910520315170288\n",
      "Loss: 1.956089973449707\n",
      "Loss: 1.9891587495803833\n",
      "Loss: 2.0182642936706543\n",
      "Loss: 2.0313031673431396\n",
      "Loss: 2.0811355113983154\n",
      "Loss: 2.001934289932251\n",
      "Loss: 1.881698727607727\n",
      "Loss: 1.8248909711837769\n",
      "Loss: 1.9137219190597534\n",
      "Loss: 2.014528274536133\n",
      "Loss: 2.010507583618164\n",
      "Loss: 1.9054263830184937\n",
      "Loss: 2.0822460651397705\n",
      "Loss: 2.0145833492279053\n",
      "Loss: 1.9483604431152344\n",
      "Loss: 1.9371939897537231\n",
      "Loss: 1.8543901443481445\n",
      "Loss: 2.0569326877593994\n",
      "Loss: 1.9202706813812256\n",
      "Loss: 1.785865068435669\n",
      "Loss: 1.8400959968566895\n",
      "Loss: 1.6524170637130737\n",
      "Loss: 1.8791298866271973\n",
      "Loss: 2.221118927001953\n",
      "Loss: 1.683150053024292\n",
      "Loss: 1.780714988708496\n",
      "Loss: 1.737726092338562\n",
      "Loss: 1.8043259382247925\n",
      "Loss: 1.783008098602295\n",
      "Loss: 1.7104649543762207\n",
      "Loss: 1.6272163391113281\n",
      "Loss: 1.9047602415084839\n",
      "Loss: 1.7783833742141724\n",
      "Loss: 1.853337287902832\n",
      "Loss: 1.6588454246520996\n",
      "Loss: 1.8793323040008545\n",
      "Loss: 1.6598048210144043\n",
      "Loss: 1.7479963302612305\n",
      "Loss: 1.7533910274505615\n",
      "Loss: 1.6609952449798584\n",
      "Loss: 1.8238153457641602\n",
      "Loss: 1.7295503616333008\n",
      "Loss: 1.7234742641448975\n",
      "Loss: 1.712705373764038\n",
      "Loss: 1.7294859886169434\n",
      "Loss: 1.5887373685836792\n",
      "Loss: 1.6970289945602417\n",
      "Loss: 1.4647438526153564\n",
      "Loss: 1.5837231874465942\n",
      "Loss: 1.7227323055267334\n",
      "Loss: 1.7101259231567383\n",
      "Loss: 1.5922799110412598\n",
      "Loss: 1.5999048948287964\n",
      "Loss: 1.5141630172729492\n",
      "Loss: 1.5411503314971924\n",
      "Loss: 1.4966918230056763\n",
      "Loss: 1.6330368518829346\n",
      "Loss: 1.5724680423736572\n",
      "Loss: 1.5503348112106323\n",
      "Loss: 1.4025945663452148\n",
      "Loss: 1.5273218154907227\n",
      "Loss: 1.3657829761505127\n",
      "Loss: 1.5263893604278564\n",
      "Loss: 1.334641695022583\n",
      "Loss: 1.2295424938201904\n",
      "Loss: 1.267838478088379\n",
      "Loss: 1.548725962638855\n",
      "Loss: 1.6616325378417969\n",
      "Loss: 1.4013352394104004\n",
      "Loss: 1.3981621265411377\n",
      "Loss: 1.1107633113861084\n",
      "Loss: 1.3585197925567627\n",
      "Loss: 1.4889461994171143\n",
      "Loss: 1.2379324436187744\n",
      "Loss: 1.1867395639419556\n",
      "Loss: 1.3110935688018799\n",
      "Loss: 1.1524823904037476\n",
      "Loss: 1.3210961818695068\n",
      "Loss: 1.4851398468017578\n",
      "Loss: 1.2800157070159912\n",
      "Loss: 1.5074262619018555\n",
      "Loss: 1.2390120029449463\n",
      "Loss: 1.3054105043411255\n",
      "Loss: 1.2241703271865845\n",
      "Loss: 1.3883168697357178\n",
      "Loss: 1.4985545873641968\n",
      "Loss: 0.6858885288238525\n",
      "Loss: 1.279676914215088\n",
      "Loss: 1.4160921573638916\n",
      "Loss: 1.4628322124481201\n",
      "Loss: 1.546753168106079\n",
      "Loss: 1.5834437608718872\n",
      "Loss: 1.1565563678741455\n",
      "Loss: 1.6235764026641846\n",
      "Loss: 1.314705729484558\n",
      "Loss: 1.3040807247161865\n",
      "Loss: 1.2006640434265137\n",
      "Loss: 1.1631582975387573\n",
      "Loss: 1.2031892538070679\n",
      "Loss: 1.5411111116409302\n",
      "Loss: 1.4168504476547241\n",
      "Loss: 1.1055779457092285\n",
      "Loss: 1.2444144487380981\n",
      "Loss: 1.1893771886825562\n",
      "Loss: 1.2135812044143677\n",
      "Loss: 1.3423545360565186\n",
      "Loss: 1.0950196981430054\n",
      "Loss: 0.7814193964004517\n",
      "Loss: 1.1775076389312744\n",
      "Loss: 0.7126182913780212\n",
      "Loss: 1.1146697998046875\n",
      "Loss: 1.0992043018341064\n",
      "Loss: 1.2741528749465942\n",
      "Loss: 1.7334457635879517\n",
      "Loss: 1.086913824081421\n",
      "Loss: 1.1098127365112305\n",
      "Loss: 0.9968332052230835\n",
      "Loss: 1.345100998878479\n",
      "Loss: 1.070443868637085\n",
      "Loss: 1.0512068271636963\n",
      "Loss: 1.0950899124145508\n",
      "Loss: 1.5635769367218018\n",
      "Loss: 1.0313856601715088\n",
      "Loss: 0.9219082593917847\n",
      "Loss: 1.2354395389556885\n",
      "Loss: 1.1208807229995728\n",
      "Loss: 1.023454189300537\n",
      "Loss: 1.2774503231048584\n",
      "Loss: 1.328856110572815\n",
      "Loss: 1.268115520477295\n",
      "Loss: 1.0151997804641724\n",
      "Loss: 1.2131671905517578\n",
      "Loss: 1.1486103534698486\n",
      "Loss: 1.2361085414886475\n",
      "Loss: 1.4148247241973877\n",
      "Loss: 0.9987619519233704\n",
      "Loss: 1.0660696029663086\n",
      "Loss: 0.8730736970901489\n",
      "Loss: 1.045799732208252\n",
      "Loss: 0.9745655059814453\n",
      "Loss: 1.0315868854522705\n",
      "Loss: 0.9739193916320801\n",
      "Loss: 0.7949603796005249\n",
      "Loss: 1.0814709663391113\n",
      "Loss: 0.855265736579895\n",
      "Loss: 0.9373264908790588\n",
      "Loss: 1.7036161422729492\n",
      "Loss: 1.0679945945739746\n",
      "Loss: 0.9661914110183716\n",
      "Loss: 1.05218505859375\n",
      "Loss: 0.9511906504631042\n",
      "Loss: 1.2659316062927246\n",
      "Loss: 0.8946241736412048\n",
      "Loss: 1.3318617343902588\n",
      "Loss: 1.054667592048645\n",
      "Loss: 0.8113230466842651\n",
      "Loss: 1.0461571216583252\n",
      "Loss: 0.8982603549957275\n",
      "Loss: 1.3558529615402222\n",
      "Loss: 0.5310840606689453\n",
      "Loss: 1.0287139415740967\n",
      "Loss: 1.0724551677703857\n",
      "Loss: 1.1461703777313232\n",
      "Loss: 0.6260324120521545\n",
      "Loss: 1.2611000537872314\n",
      "Loss: 0.9086876511573792\n",
      "Loss: 1.1687815189361572\n",
      "Loss: 1.073585033416748\n",
      "Loss: 0.902833104133606\n",
      "Loss: 0.6462347507476807\n",
      "Loss: 1.1397724151611328\n",
      "Loss: 0.9837332963943481\n",
      "Loss: 1.3188157081604004\n",
      "Loss: 1.7321137189865112\n",
      "Loss: 0.6696723699569702\n",
      "Loss: 0.9662153720855713\n",
      "Loss: 0.7296199798583984\n",
      "Loss: 0.8005334138870239\n",
      "Loss: 1.1661593914031982\n",
      "Loss: 1.3331743478775024\n",
      "Loss: 1.12386155128479\n",
      "Loss: 1.2131590843200684\n",
      "Loss: 1.1102044582366943\n",
      "Loss: 0.997837245464325\n",
      "Loss: 1.2194257974624634\n",
      "Loss: 0.5949618220329285\n",
      "Loss: 0.8716086149215698\n",
      "Loss: 1.1016030311584473\n",
      "Loss: 0.8713507056236267\n",
      "Loss: 1.168177604675293\n",
      "Loss: 0.7674294710159302\n",
      "Loss: 1.3329648971557617\n",
      "Loss: 0.8546618223190308\n",
      "Loss: 1.0456950664520264\n",
      "Loss: 1.0095964670181274\n",
      "Loss: 0.9496490955352783\n",
      "Loss: 1.4234898090362549\n",
      "Loss: 1.2384405136108398\n",
      "Loss: 1.3158642053604126\n",
      "Loss: 1.2152743339538574\n",
      "Loss: 0.7993572354316711\n",
      "Loss: 1.0591580867767334\n",
      "Loss: 0.8480392694473267\n",
      "Loss: 0.7339507341384888\n",
      "Loss: 0.7097379565238953\n",
      "Loss: 1.0829946994781494\n",
      "Loss: 0.9669787883758545\n",
      "Loss: 0.6405175924301147\n",
      "Loss: 1.3625216484069824\n",
      "Loss: 1.086966633796692\n",
      "Loss: 0.8116793632507324\n",
      "Loss: 0.42696428298950195\n",
      "Loss: 0.9397556781768799\n",
      "Loss: 0.7986218333244324\n",
      "Loss: 1.2657428979873657\n",
      "Loss: 0.8428664803504944\n",
      "Loss: 0.9266823530197144\n",
      "Loss: 0.5130528807640076\n",
      "Loss: 1.3626466989517212\n",
      "Loss: 0.8249725103378296\n",
      "Loss: 1.018622875213623\n",
      "Loss: 0.9229539632797241\n",
      "Loss: 0.6463699340820312\n",
      "Loss: 0.790322482585907\n",
      "Loss: 0.6335647106170654\n",
      "Loss: 0.9987112283706665\n",
      "Loss: 0.7083686590194702\n",
      "Loss: 0.976819634437561\n",
      "Loss: 0.8272236585617065\n",
      "Loss: 1.0401296615600586\n",
      "Loss: 0.743514358997345\n",
      "Loss: 1.0620566606521606\n",
      "Loss: 0.9785149097442627\n",
      "Loss: 0.79762864112854\n",
      "Loss: 0.8775109052658081\n",
      "Loss: 1.1399996280670166\n",
      "Loss: 1.3285963535308838\n",
      "Loss: 1.3655951023101807\n",
      "Loss: 0.5867345929145813\n",
      "Loss: 1.4530423879623413\n",
      "Loss: 0.8610281944274902\n",
      "Loss: 0.8195375800132751\n",
      "Loss: 0.7725346684455872\n",
      "Loss: 0.7987719178199768\n",
      "Loss: 0.6735510230064392\n",
      "Loss: 0.9453076720237732\n",
      "Loss: 0.5632139444351196\n",
      "Loss: 1.4155640602111816\n",
      "Loss: 0.7541636824607849\n",
      "Loss: 0.6592071056365967\n",
      "Loss: 0.6072685718536377\n",
      "Loss: 0.7698670625686646\n",
      "Loss: 0.4739179015159607\n",
      "Loss: 0.4765208959579468\n",
      "Loss: 1.160637617111206\n",
      "Loss: 0.8755085468292236\n",
      "Loss: 0.8998121619224548\n",
      "Loss: 0.7953600287437439\n",
      "Loss: 0.8417669534683228\n",
      "Loss: 0.7827935814857483\n",
      "Loss: 1.0327702760696411\n",
      "Loss: 0.7730790376663208\n",
      "Loss: 0.6823467016220093\n",
      "Loss: 0.7492543458938599\n",
      "Loss: 0.9037215113639832\n",
      "Loss: 0.2852265536785126\n",
      "Loss: 0.8280468583106995\n",
      "Loss: 0.9711205363273621\n",
      "Loss: 0.8366619348526001\n",
      "Loss: 1.040763020515442\n",
      "Loss: 0.6707750558853149\n",
      "Loss: 1.1336333751678467\n",
      "Loss: 0.8682515621185303\n",
      "Loss: 0.7700984477996826\n",
      "Loss: 0.7386464476585388\n",
      "Loss: 0.5889047384262085\n",
      "Loss: 0.9726400375366211\n",
      "Loss: 0.42902252078056335\n",
      "Loss: 0.5254815816879272\n",
      "Loss: 0.43317869305610657\n",
      "Loss: 0.7785813212394714\n",
      "Loss: 0.745266318321228\n",
      "Loss: 0.6243174076080322\n",
      "Loss: 0.6240274310112\n",
      "Loss: 0.5226029753684998\n",
      "Loss: 0.36599355936050415\n",
      "Loss: 0.6808911561965942\n",
      "Loss: 0.8843123912811279\n",
      "Loss: 1.0663256645202637\n",
      "Loss: 0.92558753490448\n",
      "Loss: 0.5168824195861816\n",
      "Loss: 0.6063334941864014\n",
      "Loss: 0.44280481338500977\n",
      "Loss: 1.0453431606292725\n",
      "Loss: 0.5870676636695862\n",
      "Loss: 0.6322107315063477\n",
      "Loss: 1.1253204345703125\n",
      "Loss: 0.797217845916748\n",
      "Loss: 0.5534956455230713\n",
      "Loss: 0.7405959367752075\n",
      "Loss: 1.1753456592559814\n",
      "Loss: 1.081657886505127\n",
      "Loss: 0.828207790851593\n",
      "Loss: 0.4641028344631195\n",
      "Loss: 0.41760367155075073\n",
      "Loss: 0.8610546588897705\n",
      "Loss: 0.812572181224823\n",
      "Loss: 0.9035931825637817\n",
      "Loss: 1.2563681602478027\n",
      "Loss: 0.9831017255783081\n",
      "Loss: 0.4645830988883972\n",
      "Loss: 0.6501805782318115\n",
      "Loss: 0.8966620564460754\n",
      "Loss: 0.5589489340782166\n",
      "Loss: 0.904009222984314\n",
      "Loss: 1.3026002645492554\n",
      "Loss: 0.47577786445617676\n",
      "Loss: 0.7077940702438354\n",
      "Loss: 0.710107147693634\n",
      "Loss: 0.6924774646759033\n",
      "Loss: 0.541813313961029\n",
      "Loss: 0.6096118092536926\n",
      "Loss: 0.4504571557044983\n",
      "Loss: 0.6368628740310669\n",
      "Loss: 1.0553351640701294\n",
      "Loss: 0.9279001951217651\n",
      "Loss: 0.8250112533569336\n",
      "Loss: 0.5741708278656006\n",
      "Loss: 0.697386622428894\n",
      "Loss: 0.4029066860675812\n",
      "Loss: 0.667364239692688\n",
      "Loss: 0.661649763584137\n",
      "Loss: 0.47700169682502747\n",
      "Loss: 0.869282066822052\n",
      "Loss: 0.6716395616531372\n",
      "Loss: 0.8663179278373718\n",
      "Loss: 1.1694695949554443\n",
      "Loss: 0.7579670548439026\n",
      "Loss: 0.5899895429611206\n",
      "Loss: 0.5674997568130493\n",
      "Loss: 1.1665153503417969\n",
      "Loss: 0.6379870176315308\n",
      "Loss: 0.6861875057220459\n",
      "Loss: 0.4858260452747345\n",
      "Loss: 0.9586029648780823\n",
      "Loss: 0.6075270175933838\n",
      "Loss: 0.7115223407745361\n",
      "Loss: 0.7291647791862488\n",
      "Loss: 0.7903907299041748\n",
      "Loss: 0.64369136095047\n",
      "Loss: 0.6552551984786987\n",
      "Loss: 0.35323676466941833\n",
      "Loss: 0.43239080905914307\n",
      "Loss: 0.29565107822418213\n",
      "Loss: 0.6876813173294067\n",
      "Loss: 0.7312981486320496\n",
      "Loss: 0.501515805721283\n",
      "Loss: 0.33247631788253784\n",
      "Loss: 0.8067829608917236\n",
      "Loss: 0.7101828455924988\n",
      "Loss: 0.8409110307693481\n",
      "Loss: 0.7547327280044556\n",
      "Loss: 0.9206651449203491\n",
      "Loss: 0.6920366287231445\n",
      "Loss: 0.8640434741973877\n",
      "Loss: 0.6289326548576355\n",
      "Loss: 0.7000570893287659\n",
      "Loss: 0.31248173117637634\n",
      "Loss: 0.924310028553009\n",
      "Loss: 0.6269048452377319\n",
      "Loss: 0.23601430654525757\n",
      "Loss: 0.5944771766662598\n",
      "Loss: 0.5934306979179382\n",
      "Loss: 0.8205889463424683\n",
      "Loss: 0.40713071823120117\n",
      "Loss: 0.4808359742164612\n",
      "Loss: 0.8750397562980652\n",
      "Loss: 0.32637473940849304\n",
      "Loss: 0.5514805316925049\n",
      "Loss: 0.4248043894767761\n",
      "Loss: 0.5464819669723511\n",
      "Loss: 0.43835127353668213\n",
      "Loss: 0.4561355710029602\n",
      "Loss: 0.7402621507644653\n",
      "Loss: 0.9398624300956726\n",
      "Loss: 0.4572024941444397\n",
      "Loss: 0.7794845104217529\n",
      "Loss: 0.724256694316864\n",
      "Loss: 0.4113656282424927\n",
      "Loss: 0.5347332954406738\n",
      "Loss: 0.37458300590515137\n",
      "Loss: 1.0232839584350586\n",
      "Loss: 0.7470751404762268\n",
      "Loss: 0.6830878257751465\n",
      "Loss: 0.2485077828168869\n",
      "Loss: 0.46048277616500854\n",
      "Loss: 0.6366132497787476\n",
      "Loss: 0.8161695003509521\n",
      "Loss: 0.3773970603942871\n",
      "Loss: 0.5772575736045837\n",
      "Loss: 0.7775099277496338\n",
      "Loss: 0.7589313387870789\n",
      "Loss: 1.2317495346069336\n",
      "Loss: 0.2706044912338257\n",
      "Loss: 0.4153793454170227\n",
      "Loss: 0.8610237240791321\n",
      "Loss: 0.8149375319480896\n",
      "Loss: 0.6127216815948486\n",
      "Loss: 0.3820344805717468\n",
      "Loss: 0.6728016138076782\n",
      "Loss: 0.38179731369018555\n",
      "Loss: 0.46267151832580566\n",
      "Loss: 0.5967594385147095\n",
      "Loss: 0.6170477271080017\n",
      "Loss: 0.2615220546722412\n",
      "Loss: 0.38199710845947266\n",
      "Loss: 0.4445435702800751\n",
      "Loss: 0.47035446763038635\n",
      "Loss: 0.5355996489524841\n",
      "Loss: 1.0343574285507202\n",
      "Loss: 0.5190109014511108\n",
      "Loss: 0.46879783272743225\n",
      "Loss: 0.6577423810958862\n",
      "Loss: 0.5964319705963135\n",
      "Loss: 0.8279844522476196\n",
      "Loss: 1.0898246765136719\n",
      "Loss: 0.6782469153404236\n",
      "Loss: 0.34744352102279663\n",
      "Loss: 0.6582409143447876\n",
      "Loss: 0.6529967784881592\n",
      "Loss: 0.5907940864562988\n",
      "Loss: 1.0655256509780884\n",
      "Loss: 0.5838813781738281\n",
      "Loss: 0.43940067291259766\n",
      "Loss: 0.9240624904632568\n",
      "Loss: 0.8124523162841797\n",
      "Loss: 0.4944542348384857\n",
      "Loss: 1.283592700958252\n",
      "Loss: 0.4012099802494049\n",
      "Loss: 0.8493773341178894\n",
      "Loss: 0.49285271763801575\n",
      "Loss: 0.1295948028564453\n",
      "Loss: 0.7734814882278442\n",
      "Loss: 0.24570494890213013\n",
      "Loss: 0.34861040115356445\n",
      "Loss: 0.5184259414672852\n",
      "Loss: 0.700249433517456\n",
      "Loss: 0.855027973651886\n",
      "Loss: 0.6167360544204712\n",
      "Loss: 0.21954244375228882\n",
      "Loss: 0.9854791164398193\n",
      "Loss: 0.41859906911849976\n",
      "Loss: 0.6111643314361572\n",
      "Loss: 0.6729137897491455\n",
      "Loss: 0.3721533715724945\n",
      "Loss: 0.640623152256012\n",
      "Loss: 0.31084632873535156\n",
      "Loss: 0.4220081567764282\n",
      "Loss: 0.4550888240337372\n",
      "Loss: 0.5013740062713623\n",
      "Loss: 0.5554745197296143\n",
      "Loss: 0.3547857403755188\n",
      "Loss: 0.5202046632766724\n",
      "Loss: 0.5169523358345032\n",
      "Loss: 0.1518651843070984\n",
      "Loss: 0.4038310647010803\n",
      "Loss: 0.5842140316963196\n",
      "Loss: 0.5907636284828186\n",
      "Loss: 0.789942741394043\n",
      "Loss: 0.4625087380409241\n",
      "Loss: 0.2306509166955948\n",
      "Loss: 0.49908363819122314\n",
      "Loss: 0.3885233998298645\n",
      "Loss: 0.2943769097328186\n",
      "Loss: 0.39039021730422974\n",
      "Loss: 0.5233138203620911\n",
      "Loss: 0.4984743595123291\n",
      "Loss: 0.49003180861473083\n",
      "Loss: 0.5217207670211792\n",
      "Loss: 0.7890732288360596\n",
      "Loss: 0.3924254775047302\n",
      "Loss: 0.6988853812217712\n",
      "Loss: 0.6608998775482178\n",
      "Loss: 0.4789966940879822\n",
      "Loss: 0.40580564737319946\n",
      "Loss: 0.402368426322937\n",
      "Loss: 1.0455654859542847\n",
      "Loss: 0.3305373191833496\n",
      "Loss: 0.9441136717796326\n",
      "Loss: 0.5235310792922974\n",
      "Loss: 0.8506683111190796\n",
      "Loss: 0.4330762028694153\n",
      "Loss: 0.4015319347381592\n",
      "Loss: 0.5014810562133789\n",
      "Loss: 0.17073707282543182\n",
      "Loss: 0.7201145887374878\n",
      "Loss: 0.7127454280853271\n",
      "Loss: 0.4246253967285156\n",
      "Loss: 0.4450514614582062\n",
      "Loss: 0.5775306820869446\n",
      "Loss: 0.11677797138690948\n",
      "Loss: 0.317023366689682\n",
      "Loss: 0.6470171213150024\n",
      "Loss: 0.7727846503257751\n",
      "Loss: 0.21619105339050293\n",
      "Loss: 0.7732187509536743\n",
      "Loss: 0.8459830284118652\n",
      "Loss: 0.4182562828063965\n",
      "Loss: 0.4722634553909302\n",
      "Loss: 0.43938907980918884\n",
      "Loss: 0.3771877586841583\n",
      "Loss: 0.40631937980651855\n",
      "Loss: 0.36671262979507446\n",
      "Loss: 0.36427927017211914\n",
      "Loss: 0.39037469029426575\n",
      "Loss: 0.5796548128128052\n",
      "Loss: 0.18080902099609375\n",
      "Loss: 0.20330235362052917\n",
      "Loss: 0.6093128323554993\n",
      "Loss: 0.8400362133979797\n",
      "Loss: 0.5480378866195679\n",
      "Loss: 0.5933070182800293\n",
      "Loss: 0.26534974575042725\n",
      "Loss: 0.2890219986438751\n",
      "Loss: 0.8333260416984558\n",
      "Loss: 0.47286760807037354\n",
      "Loss: 0.8413937091827393\n",
      "Loss: 0.7737337946891785\n",
      "Loss: 0.540165364742279\n",
      "Loss: 0.480535089969635\n",
      "Loss: 0.3639574944972992\n",
      "Loss: 0.846844494342804\n",
      "Loss: 0.2297355830669403\n",
      "Loss: 0.3738762140274048\n",
      "Loss: 0.764321506023407\n",
      "Loss: 0.5347811579704285\n",
      "Loss: 0.8830759525299072\n",
      "Loss: 0.6343247294425964\n",
      "Loss: 0.3623042106628418\n",
      "Loss: 0.44407492876052856\n",
      "Loss: 0.4872044622898102\n",
      "Loss: 0.502440333366394\n",
      "Loss: 0.5250133275985718\n",
      "Loss: 0.4695736765861511\n",
      "Loss: 0.5156593918800354\n",
      "Loss: 0.6139776706695557\n",
      "Loss: 0.4926947057247162\n",
      "Loss: 0.3298627436161041\n",
      "Loss: 0.8364778757095337\n",
      "Loss: 0.4061838388442993\n",
      "Loss: 0.39650678634643555\n",
      "Loss: 0.1896967887878418\n",
      "Loss: 0.7469289898872375\n",
      "Loss: 0.3716827630996704\n",
      "Loss: 0.4263589680194855\n",
      "Loss: 0.2320132702589035\n",
      "Loss: 0.33809587359428406\n",
      "Loss: 0.3431648313999176\n",
      "Loss: 0.809971272945404\n",
      "Loss: 0.24287986755371094\n",
      "Loss: 0.5706014037132263\n",
      "Loss: 0.6622580289840698\n",
      "Loss: 0.235548734664917\n",
      "Loss: 0.3804172873497009\n",
      "Loss: 0.256531298160553\n",
      "Loss: 0.6542702913284302\n",
      "Loss: 0.2373759150505066\n",
      "Loss: 0.19372422993183136\n",
      "Loss: 0.38167804479599\n",
      "Loss: 0.298456609249115\n",
      "Loss: 0.6125496625900269\n",
      "Loss: 0.4930613338947296\n",
      "Loss: 0.3370369076728821\n",
      "Loss: 0.5999380350112915\n",
      "Loss: 0.48513519763946533\n",
      "Loss: 0.6383084654808044\n",
      "Loss: 0.5968235731124878\n",
      "Loss: 0.21357515454292297\n",
      "Loss: 0.6078119277954102\n",
      "Loss: 0.4517393410205841\n",
      "Loss: 0.5481506586074829\n",
      "Loss: 0.4677930474281311\n",
      "Loss: 0.651513934135437\n",
      "Loss: 0.6847994923591614\n",
      "Loss: 0.405365526676178\n",
      "Loss: 0.2161998450756073\n",
      "Loss: 0.534882664680481\n",
      "Loss: 0.43058910965919495\n",
      "Loss: 0.4455263316631317\n",
      "Loss: 0.8393861651420593\n",
      "Loss: 1.0075883865356445\n",
      "Loss: 0.2651963233947754\n",
      "Loss: 0.2752113342285156\n",
      "Loss: 0.30612048506736755\n",
      "Loss: 0.6259106397628784\n",
      "Loss: 0.3136790990829468\n",
      "Loss: 0.5762306451797485\n",
      "Loss: 0.9386114478111267\n",
      "Loss: 0.18725812435150146\n",
      "Loss: 0.37118905782699585\n",
      "Loss: 0.3494112193584442\n",
      "Loss: 0.21935966610908508\n",
      "Loss: 0.34080809354782104\n",
      "Loss: 0.44544121623039246\n",
      "Loss: 0.2664080262184143\n",
      "Loss: 0.1295696496963501\n",
      "Loss: 0.3834429085254669\n",
      "Loss: 0.5989782810211182\n",
      "Loss: 0.36267703771591187\n",
      "Loss: 0.340141236782074\n",
      "Loss: 0.6132667064666748\n",
      "Loss: 0.5547488331794739\n",
      "Loss: 0.4289655089378357\n",
      "Loss: 0.1996556520462036\n",
      "Loss: 0.8177816867828369\n",
      "Loss: 1.0297584533691406\n",
      "Loss: 0.5078191161155701\n",
      "Loss: 0.5312032699584961\n",
      "Loss: 0.17145831882953644\n",
      "Loss: 0.17054665088653564\n",
      "Loss: 0.45928704738616943\n",
      "Loss: 0.18596869707107544\n",
      "Loss: 0.2839859426021576\n",
      "Loss: 0.26150354743003845\n",
      "Loss: 0.677844226360321\n",
      "Loss: 0.36732640862464905\n",
      "Loss: 0.38457244634628296\n",
      "Loss: 0.34322506189346313\n",
      "Loss: 0.5150463581085205\n",
      "Loss: 0.16390928626060486\n",
      "Loss: 0.20117948949337006\n",
      "Loss: 0.35184141993522644\n",
      "Loss: 0.19056710600852966\n",
      "Loss: 0.6249275803565979\n",
      "Loss: 0.2905076742172241\n",
      "Loss: 0.572625458240509\n",
      "Loss: 0.5115141868591309\n",
      "Loss: 0.5349361300468445\n",
      "Loss: 0.19609935581684113\n",
      "Loss: 0.5299848318099976\n",
      "Loss: 0.5586732625961304\n",
      "Loss: 1.322335958480835\n",
      "Loss: 0.7490880489349365\n",
      "Loss: 0.44642215967178345\n",
      "Loss: 0.33693552017211914\n",
      "Loss: 0.20097418129444122\n",
      "Loss: 0.3326840400695801\n",
      "Loss: 0.3405565619468689\n",
      "Loss: 0.3645814061164856\n",
      "Loss: 0.5291956663131714\n",
      "Loss: 0.3926917314529419\n",
      "Loss: 0.27812016010284424\n",
      "Loss: 0.4277345836162567\n",
      "Loss: 0.3262792229652405\n",
      "Loss: 0.3658316433429718\n",
      "Loss: 0.2579326629638672\n",
      "Loss: 0.6009975075721741\n",
      "Loss: 0.24377480149269104\n",
      "Loss: 0.5367329120635986\n",
      "Loss: 0.5038989782333374\n",
      "Loss: 0.17948654294013977\n",
      "Loss: 0.3617076277732849\n",
      "Loss: 0.24378691613674164\n",
      "Loss: 0.37106025218963623\n",
      "Loss: 0.5590640306472778\n",
      "Loss: 0.6821531057357788\n",
      "Loss: 0.7868545651435852\n",
      "Loss: 0.9235966205596924\n",
      "Loss: 0.7355058789253235\n",
      "Loss: 0.5965898633003235\n",
      "Loss: 0.26236313581466675\n",
      "Loss: 0.2875460386276245\n",
      "Loss: 0.507393479347229\n",
      "Loss: 0.3542242646217346\n",
      "Loss: 0.6467224955558777\n",
      "Loss: 0.4739260673522949\n",
      "Loss: 0.5150020718574524\n",
      "Loss: 0.6500554084777832\n",
      "Loss: 0.6456194519996643\n",
      "Loss: 0.6449970006942749\n",
      "Loss: 1.0006296634674072\n",
      "Loss: 0.4399326741695404\n",
      "Loss: 0.5360891819000244\n",
      "Loss: 0.5537526607513428\n",
      "Loss: 0.45682716369628906\n",
      "Loss: 0.7875102162361145\n",
      "Loss: 0.4627719223499298\n",
      "Loss: 0.46927204728126526\n",
      "Loss: 0.5499793291091919\n",
      "Loss: 0.32164156436920166\n",
      "Loss: 0.38637039065361023\n",
      "Loss: 0.4487987756729126\n",
      "Loss: 0.7029358744621277\n",
      "Loss: 0.5327631831169128\n",
      "Loss: 0.4234662652015686\n",
      "Loss: 0.819462239742279\n",
      "Loss: 0.3418366312980652\n",
      "Loss: 1.1316176652908325\n",
      "Loss: 0.5151245594024658\n",
      "Loss: 0.4436255693435669\n",
      "Loss: 0.45459794998168945\n",
      "Loss: 0.4586859941482544\n",
      "Loss: 0.42344045639038086\n",
      "Loss: 0.4078328609466553\n",
      "Loss: 0.9324005842208862\n",
      "Loss: 0.38907870650291443\n",
      "Loss: 0.3386024832725525\n",
      "Loss: 0.4260549247264862\n",
      "Loss: 0.22128711640834808\n",
      "Loss: 0.5839829444885254\n",
      "Loss: 0.8286073207855225\n",
      "Loss: 0.26567965745925903\n",
      "Loss: 0.3896525502204895\n",
      "Loss: 0.5599960088729858\n",
      "Loss: 0.22897066175937653\n",
      "Loss: 0.5489320158958435\n",
      "Loss: 0.4618757367134094\n",
      "Loss: 0.13153775036334991\n",
      "Loss: 0.5928754210472107\n",
      "Loss: 0.6118854284286499\n",
      "Loss: 0.5489022135734558\n",
      "Loss: 0.35993120074272156\n",
      "Loss: 0.43460944294929504\n",
      "Loss: 0.148542582988739\n",
      "Loss: 0.5962879657745361\n",
      "Loss: 0.44055700302124023\n",
      "Loss: 0.6861151456832886\n",
      "Loss: 0.7670727372169495\n",
      "Loss: 0.25957798957824707\n",
      "Loss: 0.743960440158844\n",
      "Loss: 0.2238491177558899\n",
      "Loss: 0.13252772390842438\n",
      "Loss: 0.2202223241329193\n",
      "Loss: 0.357466459274292\n",
      "Loss: 0.34108835458755493\n",
      "Loss: 0.683297872543335\n",
      "Loss: 0.7154594659805298\n",
      "Loss: 0.1847955286502838\n",
      "Loss: 0.2953328490257263\n",
      "Loss: 0.286901593208313\n",
      "Loss: 0.30248045921325684\n",
      "Loss: 0.8790333271026611\n",
      "Loss: 0.30651023983955383\n",
      "Loss: 0.22402942180633545\n",
      "Loss: 0.5867226123809814\n",
      "Loss: 0.5360208749771118\n",
      "Loss: 0.4655318260192871\n",
      "Loss: 0.10059578716754913\n",
      "Loss: 0.052671879529953\n",
      "Loss: 0.5676351189613342\n",
      "Loss: 0.5369238257408142\n",
      "Loss: 0.4651530086994171\n",
      "Loss: 0.33831140398979187\n",
      "Loss: 0.4453333914279938\n",
      "Loss: 0.2218349128961563\n",
      "Loss: 0.3449999988079071\n",
      "Loss: 0.23462049663066864\n",
      "Loss: 0.16259461641311646\n",
      "Loss: 0.4946849048137665\n",
      "Loss: 0.5400289297103882\n",
      "Loss: 0.7492654323577881\n",
      "Loss: 0.476177453994751\n",
      "Loss: 0.19909510016441345\n",
      "Loss: 0.48656904697418213\n",
      "Loss: 0.46608105301856995\n",
      "Loss: 0.3970527648925781\n",
      "Loss: 0.2705773711204529\n",
      "Loss: 0.48371076583862305\n",
      "Loss: 0.3957445025444031\n",
      "Loss: 0.377287358045578\n",
      "Loss: 0.4337608218193054\n",
      "Loss: 0.279813289642334\n",
      "Loss: 0.2839902937412262\n",
      "Loss: 0.42257437109947205\n",
      "Loss: 0.7182959318161011\n",
      "Loss: 0.5241400003433228\n",
      "Loss: 0.3051111102104187\n",
      "Loss: 0.5952918529510498\n",
      "Loss: 0.6616910696029663\n",
      "Loss: 0.6983280777931213\n",
      "Loss: 0.2565445899963379\n",
      "Loss: 0.4378918707370758\n",
      "Loss: 0.4142395555973053\n",
      "Loss: 0.3568842113018036\n",
      "Loss: 0.8329078555107117\n",
      "Loss: 0.36042970418930054\n",
      "Loss: 0.4094476103782654\n",
      "Loss: 0.48507940769195557\n",
      "Loss: 0.5305269360542297\n",
      "Loss: 0.39666426181793213\n",
      "Loss: 0.3011186718940735\n",
      "Loss: 0.3402745723724365\n",
      "Loss: 0.2919163405895233\n",
      "Loss: 0.447992205619812\n",
      "Loss: 0.2897428274154663\n",
      "Loss: 0.4430020749568939\n",
      "Loss: 0.22182461619377136\n",
      "Loss: 0.3692775070667267\n",
      "Loss: 0.24438372254371643\n",
      "Loss: 0.40454548597335815\n",
      "Loss: 0.5939019918441772\n",
      "Loss: 0.3012661039829254\n",
      "Loss: 0.2828853726387024\n",
      "Loss: 0.4087604880332947\n",
      "Loss: 0.5328335165977478\n",
      "Loss: 0.3350873589515686\n",
      "Loss: 0.5799086093902588\n",
      "Loss: 0.2890389561653137\n",
      "Loss: 0.20621000230312347\n",
      "Loss: 0.31303834915161133\n",
      "Loss: 0.5239995718002319\n",
      "Loss: 0.9240754246711731\n",
      "Loss: 0.32576850056648254\n",
      "Loss: 0.07310177385807037\n",
      "Loss: 0.3454197347164154\n",
      "Loss: 0.22068320214748383\n",
      "Loss: 0.19704271852970123\n",
      "Loss: 0.22205829620361328\n",
      "Loss: 0.32149502635002136\n",
      "Loss: 0.42396634817123413\n",
      "Loss: 0.5406589508056641\n",
      "Loss: 0.6921118497848511\n",
      "Loss: 0.8852399587631226\n",
      "Loss: 0.23922979831695557\n",
      "Loss: 0.4016806483268738\n",
      "Loss: 0.23243772983551025\n",
      "Loss: 0.22620990872383118\n",
      "Loss: 0.14618578553199768\n",
      "Loss: 0.09534434974193573\n",
      "Loss: 0.449088990688324\n",
      "Loss: 0.2702608108520508\n",
      "Loss: 0.2784495949745178\n",
      "Loss: 0.6435542106628418\n",
      "Loss: 0.46120238304138184\n",
      "Loss: 0.413765013217926\n",
      "Loss: 0.2915595769882202\n",
      "Loss: 0.26045095920562744\n",
      "Loss: 0.7379430532455444\n",
      "Loss: 0.21059143543243408\n",
      "Loss: 0.2754650115966797\n",
      "Loss: 0.43931540846824646\n",
      "Loss: 0.26495271921157837\n",
      "Loss: 0.5729281902313232\n",
      "Loss: 0.42890292406082153\n",
      "Loss: 0.709587574005127\n",
      "Loss: 0.18809257447719574\n",
      "Loss: 0.6461085677146912\n",
      "Loss: 0.6574684977531433\n",
      "Loss: 0.20444995164871216\n",
      "Loss: 0.6005886793136597\n",
      "Loss: 0.10021395981311798\n",
      "Loss: 0.09731407463550568\n",
      "Loss: 0.21542921662330627\n",
      "Loss: 0.4979473948478699\n",
      "Loss: 0.5956997871398926\n",
      "Loss: 0.28069812059402466\n",
      "Loss: 0.35202834010124207\n",
      "Loss: 0.15308219194412231\n",
      "Loss: 0.36944884061813354\n",
      "Loss: 0.21150419116020203\n",
      "Loss: 0.22933046519756317\n",
      "Loss: 0.27865856885910034\n",
      "Loss: 0.3452298641204834\n",
      "Loss: 0.43358567357063293\n",
      "Loss: 0.2613985538482666\n",
      "Loss: 0.2736762464046478\n",
      "Loss: 0.0637621134519577\n",
      "Loss: 0.4207499325275421\n",
      "Loss: 0.3583086133003235\n",
      "Loss: 0.5732852220535278\n",
      "Loss: 0.13688978552818298\n",
      "Loss: 0.5014194846153259\n",
      "Loss: 0.29001274704933167\n",
      "Loss: 0.34163418412208557\n",
      "Loss: 0.176633819937706\n",
      "Loss: 0.1390584260225296\n",
      "Loss: 0.10506195574998856\n",
      "Loss: 0.23233819007873535\n",
      "Loss: 0.39619186520576477\n",
      "Loss: 0.20782555639743805\n",
      "Loss: 0.19881373643875122\n",
      "Loss: 0.40219277143478394\n",
      "Loss: 0.5951695442199707\n",
      "Loss: 0.39433416724205017\n",
      "Loss: 0.19842909276485443\n",
      "Loss: 0.23907190561294556\n",
      "Loss: 0.7007923126220703\n",
      "Loss: 0.3923218846321106\n",
      "Loss: 0.3663637936115265\n",
      "Loss: 0.30510687828063965\n",
      "Loss: 0.07074513286352158\n",
      "Loss: 0.34261223673820496\n",
      "Loss: 0.2678636312484741\n",
      "Loss: 0.18634925782680511\n",
      "Loss: 0.16313135623931885\n",
      "Loss: 0.30325761437416077\n",
      "Loss: 0.19905497133731842\n",
      "Loss: 0.7624927759170532\n",
      "Loss: 0.06853881478309631\n",
      "Loss: 0.32841384410858154\n",
      "Loss: 0.6329188346862793\n",
      "Loss: 0.4159274697303772\n",
      "Loss: 0.6605156660079956\n",
      "Loss: 0.4652099311351776\n",
      "Loss: 0.2682986259460449\n",
      "Loss: 0.09836295992136002\n",
      "Loss: 0.09700218588113785\n",
      "Loss: 0.3083479106426239\n",
      "Loss: 0.14106041193008423\n",
      "Loss: 0.7277665734291077\n",
      "Loss: 0.6371428370475769\n",
      "Loss: 0.6736006736755371\n",
      "Loss: 0.12154915928840637\n",
      "Loss: 0.44905757904052734\n",
      "Loss: 0.3574444055557251\n",
      "Loss: 0.14148321747779846\n",
      "Loss: 0.7882585525512695\n",
      "Loss: 0.2238878458738327\n",
      "Loss: 0.28628870844841003\n",
      "Loss: 0.4675590395927429\n",
      "Loss: 0.7280009984970093\n",
      "Loss: 0.5539379715919495\n",
      "Loss: 0.3962535262107849\n",
      "Loss: 0.06790188699960709\n",
      "Loss: 0.2156962752342224\n",
      "Loss: 0.5218302607536316\n",
      "Loss: 0.2959577143192291\n",
      "Loss: 0.4375095069408417\n",
      "Loss: 0.11040978878736496\n",
      "Loss: 0.7186538577079773\n",
      "Loss: 0.5344223380088806\n",
      "Loss: 0.18922336399555206\n",
      "Loss: 0.4070931673049927\n",
      "Loss: 0.09782843291759491\n",
      "Loss: 0.5411717891693115\n",
      "Loss: 0.5948342680931091\n",
      "Loss: 0.35384976863861084\n",
      "Loss: 0.40750420093536377\n",
      "Loss: 0.12185503542423248\n",
      "Loss: 0.3918270766735077\n",
      "Loss: 0.2593005299568176\n",
      "Loss: 0.09987697750329971\n",
      "Loss: 0.10287496447563171\n",
      "Loss: 0.33598676323890686\n",
      "Loss: 0.3517411947250366\n",
      "Loss: 0.36061781644821167\n",
      "Loss: 0.42151594161987305\n",
      "Loss: 0.34577667713165283\n",
      "Loss: 0.3367898464202881\n",
      "Loss: 0.721686840057373\n",
      "Loss: 0.12838959693908691\n",
      "Loss: 0.06886758655309677\n",
      "Loss: 0.27029216289520264\n",
      "Loss: 0.396548330783844\n",
      "Loss: 0.3004015386104584\n",
      "Loss: 0.5793053507804871\n",
      "Loss: 0.25100088119506836\n",
      "Loss: 0.3187657594680786\n",
      "Loss: 0.1748420149087906\n",
      "Loss: 0.3574010133743286\n",
      "Loss: 0.061471596360206604\n",
      "Loss: 0.6656391620635986\n",
      "Loss: 0.3755631148815155\n",
      "Loss: 0.516639769077301\n",
      "Loss: 0.4512477517127991\n",
      "Loss: 0.5756126642227173\n",
      "Loss: 0.189371719956398\n",
      "Loss: 0.3806992173194885\n",
      "Loss: 0.21595630049705505\n",
      "Loss: 0.10828778892755508\n",
      "Loss: 0.2943353056907654\n",
      "Loss: 0.41056710481643677\n",
      "Loss: 0.7877346277236938\n",
      "Loss: 0.5739785432815552\n",
      "Loss: 0.5724179148674011\n",
      "Loss: 0.23935382068157196\n",
      "Loss: 0.36268168687820435\n",
      "Loss: 0.22749701142311096\n",
      "Loss: 0.4714963436126709\n",
      "Loss: 0.5053102970123291\n",
      "Loss: 0.13502882421016693\n",
      "Loss: 0.514455258846283\n",
      "Loss: 0.4031727910041809\n",
      "Loss: 0.27114081382751465\n",
      "Loss: 0.16694384813308716\n",
      "Loss: 0.20639625191688538\n",
      "Loss: 0.6065993309020996\n",
      "Loss: 0.23371589183807373\n",
      "Loss: 0.08996409177780151\n",
      "Loss: 0.3079570531845093\n",
      "Loss: 0.09748288989067078\n",
      "Loss: 0.5444011688232422\n",
      "Loss: 0.05441153049468994\n",
      "Loss: 0.14303408563137054\n",
      "Loss: 0.4232970178127289\n",
      "Loss: 0.04956614971160889\n",
      "Loss: 0.14450469613075256\n",
      "Loss: 0.09940430521965027\n",
      "Loss: 0.04639587551355362\n",
      "Loss: 0.2989144027233124\n",
      "Loss: 0.5856930613517761\n",
      "Loss: 0.21180403232574463\n",
      "Loss: 0.260489284992218\n",
      "Loss: 0.03373756259679794\n",
      "Loss: 0.7326954007148743\n",
      "Loss: 0.04761316627264023\n",
      "Loss: 0.3026730716228485\n",
      "Loss: 0.3529225289821625\n",
      "Loss: 0.19552968442440033\n",
      "Loss: 0.3414418697357178\n",
      "Loss: 0.2372855544090271\n",
      "Loss: 0.21206434071063995\n",
      "Loss: 0.18793272972106934\n",
      "Loss: 0.4563000202178955\n",
      "Loss: 0.26958829164505005\n",
      "Loss: 0.43580591678619385\n",
      "Loss: 0.25456342101097107\n",
      "Loss: 0.17160925269126892\n",
      "Loss: 0.4667155146598816\n",
      "Loss: 0.2739608585834503\n",
      "Loss: 0.1893116980791092\n",
      "Loss: 0.37472355365753174\n",
      "Loss: 0.33233553171157837\n",
      "Loss: 0.8660324811935425\n",
      "Loss: 0.09646490961313248\n",
      "Loss: 0.23850227892398834\n",
      "Loss: 0.19540658593177795\n",
      "Loss: 0.14017526805400848\n",
      "Loss: 0.33400407433509827\n",
      "Loss: 0.7185479998588562\n",
      "Loss: 0.5230008363723755\n",
      "Loss: 0.17228561639785767\n",
      "Loss: 0.11805079877376556\n",
      "Loss: 0.20235471427440643\n",
      "Loss: 0.5473214983940125\n",
      "Loss: 0.34173616766929626\n",
      "Loss: 0.10998275130987167\n",
      "Loss: 0.09593699872493744\n",
      "Loss: 0.2720831334590912\n",
      "Loss: 0.1585896909236908\n",
      "Loss: 0.1509070247411728\n",
      "Loss: 0.13072042167186737\n",
      "Loss: 0.3648155927658081\n",
      "Loss: 0.48876798152923584\n",
      "Loss: 0.18137584626674652\n",
      "Epoch 2/3\n",
      "Loss: 0.21796144545078278\n",
      "Loss: 0.30737367272377014\n",
      "Loss: 0.2828998565673828\n",
      "Loss: 0.49025270342826843\n",
      "Loss: 0.4425850510597229\n",
      "Loss: 0.5344879627227783\n",
      "Loss: 0.26572567224502563\n",
      "Loss: 0.5065454840660095\n",
      "Loss: 0.1658029556274414\n",
      "Loss: 0.5029725432395935\n",
      "Loss: 0.5098240971565247\n",
      "Loss: 0.13501247763633728\n",
      "Loss: 0.5157873034477234\n",
      "Loss: 0.31590914726257324\n",
      "Loss: 0.13007231056690216\n",
      "Loss: 0.6578702926635742\n",
      "Loss: 0.10920943319797516\n",
      "Loss: 0.26017993688583374\n",
      "Loss: 0.23147010803222656\n",
      "Loss: 0.2894790470600128\n",
      "Loss: 0.49447882175445557\n",
      "Loss: 0.14649388194084167\n",
      "Loss: 0.19540202617645264\n",
      "Loss: 0.5719786286354065\n",
      "Loss: 0.22976002097129822\n",
      "Loss: 0.2668820023536682\n",
      "Loss: 0.20907288789749146\n",
      "Loss: 0.038131121546030045\n",
      "Loss: 0.06568648666143417\n",
      "Loss: 0.31573420763015747\n",
      "Loss: 0.03893083706498146\n",
      "Loss: 0.11527203023433685\n",
      "Loss: 0.23499354720115662\n",
      "Loss: 0.07151526212692261\n",
      "Loss: 0.5046502947807312\n",
      "Loss: 0.1819758415222168\n",
      "Loss: 0.4519314765930176\n",
      "Loss: 0.42358648777008057\n",
      "Loss: 0.31283092498779297\n",
      "Loss: 0.18268615007400513\n",
      "Loss: 0.18248172104358673\n",
      "Loss: 0.15130510926246643\n",
      "Loss: 0.22560317814350128\n",
      "Loss: 0.7048192024230957\n",
      "Loss: 0.560555100440979\n",
      "Loss: 0.3348129391670227\n",
      "Loss: 0.0702357292175293\n",
      "Loss: 0.3253873586654663\n",
      "Loss: 0.32539594173431396\n",
      "Loss: 0.24931038916110992\n",
      "Loss: 0.07755675911903381\n",
      "Loss: 0.2677229642868042\n",
      "Loss: 0.07442833483219147\n",
      "Loss: 0.31741324067115784\n",
      "Loss: 0.11555805057287216\n",
      "Loss: 0.14977863430976868\n",
      "Loss: 0.34628182649612427\n",
      "Loss: 0.5041680932044983\n",
      "Loss: 0.14465337991714478\n",
      "Loss: 0.38437938690185547\n",
      "Loss: 0.11564159393310547\n",
      "Loss: 0.22598594427108765\n",
      "Loss: 0.166359081864357\n",
      "Loss: 0.1076778918504715\n",
      "Loss: 0.21583423018455505\n",
      "Loss: 0.029657578095793724\n",
      "Loss: 0.4503434896469116\n",
      "Loss: 0.2507781982421875\n",
      "Loss: 0.22587153315544128\n",
      "Loss: 0.06502730399370193\n",
      "Loss: 0.04466771334409714\n",
      "Loss: 0.5100811719894409\n",
      "Loss: 0.6365097761154175\n",
      "Loss: 0.05929984152317047\n",
      "Loss: 0.10487401485443115\n",
      "Loss: 0.22555625438690186\n",
      "Loss: 0.23901432752609253\n",
      "Loss: 0.7108339667320251\n",
      "Loss: 0.4915287494659424\n",
      "Loss: 0.449161171913147\n",
      "Loss: 0.11213626712560654\n",
      "Loss: 0.09373408555984497\n",
      "Loss: 0.39765509963035583\n",
      "Loss: 0.3774833679199219\n",
      "Loss: 0.28032663464546204\n",
      "Loss: 0.05291353166103363\n",
      "Loss: 0.19570983946323395\n",
      "Loss: 0.17685404419898987\n",
      "Loss: 0.2588549852371216\n",
      "Loss: 0.3499540388584137\n",
      "Loss: 0.2055337280035019\n",
      "Loss: 0.07678566873073578\n",
      "Loss: 0.5516226887702942\n",
      "Loss: 0.22786374390125275\n",
      "Loss: 0.09435969591140747\n",
      "Loss: 0.39239200949668884\n",
      "Loss: 0.44173115491867065\n",
      "Loss: 0.2072824239730835\n",
      "Loss: 0.7415804266929626\n",
      "Loss: 0.3284628987312317\n",
      "Loss: 0.11520686745643616\n",
      "Loss: 0.1285686194896698\n",
      "Loss: 0.15514841675758362\n",
      "Loss: 0.31764036417007446\n",
      "Loss: 0.15639787912368774\n",
      "Loss: 0.1550184041261673\n",
      "Loss: 0.22146639227867126\n",
      "Loss: 0.37872540950775146\n",
      "Loss: 0.20907767117023468\n",
      "Loss: 0.04426484927535057\n",
      "Loss: 0.12347501516342163\n",
      "Loss: 0.357219398021698\n",
      "Loss: 0.35376930236816406\n",
      "Loss: 0.05556485429406166\n",
      "Loss: 0.15165916085243225\n",
      "Loss: 0.17724336683750153\n",
      "Loss: 0.7610593438148499\n",
      "Loss: 0.048419199883937836\n",
      "Loss: 0.33971840143203735\n",
      "Loss: 0.10774776339530945\n",
      "Loss: 0.24380940198898315\n",
      "Loss: 0.043167777359485626\n",
      "Loss: 0.3193330764770508\n",
      "Loss: 0.14735694229602814\n",
      "Loss: 0.20258846879005432\n",
      "Loss: 0.5434948205947876\n",
      "Loss: 0.7928285598754883\n",
      "Loss: 0.11566855013370514\n",
      "Loss: 0.6699774265289307\n",
      "Loss: 0.29846861958503723\n",
      "Loss: 0.2114691436290741\n",
      "Loss: 0.12079527974128723\n",
      "Loss: 0.802445113658905\n",
      "Loss: 0.2932516038417816\n",
      "Loss: 0.3831518590450287\n",
      "Loss: 0.25016242265701294\n",
      "Loss: 0.03922213613986969\n",
      "Loss: 0.40139538049697876\n",
      "Loss: 0.052026864141225815\n",
      "Loss: 0.1280444860458374\n",
      "Loss: 0.34157195687294006\n",
      "Loss: 0.05813096836209297\n",
      "Loss: 0.07347331196069717\n",
      "Loss: 0.1775890290737152\n",
      "Loss: 0.2525404095649719\n",
      "Loss: 0.17662429809570312\n",
      "Loss: 0.9077163338661194\n",
      "Loss: 0.4072653651237488\n",
      "Loss: 0.1981847882270813\n",
      "Loss: 0.32975977659225464\n",
      "Loss: 0.06701266020536423\n",
      "Loss: 0.03467230871319771\n",
      "Loss: 0.28032878041267395\n",
      "Loss: 0.25817516446113586\n",
      "Loss: 0.2531185448169708\n",
      "Loss: 0.1738320291042328\n",
      "Loss: 0.14522160589694977\n",
      "Loss: 0.2854042053222656\n",
      "Loss: 0.09376217424869537\n",
      "Loss: 0.20523224771022797\n",
      "Loss: 0.37245211005210876\n",
      "Loss: 0.15343867242336273\n",
      "Loss: 0.3357814848423004\n",
      "Loss: 0.1684836745262146\n",
      "Loss: 0.10774967074394226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     73\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 74\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\thebl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load your dataset\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "# Ensure df has 'stemmed_text' and 'label_angka' columns\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['stemmed_text'])\n",
    "\n",
    "# Apply SMOTE on TF-IDF features\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, df['label_angka'])\n",
    "\n",
    "# Tokenize text using BERT tokenizer\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "\n",
    "# Assuming you convert X_resampled back to text here (not shown)\n",
    "\n",
    "# Tokenize text\n",
    "tokenized_datasets = [tokenize_function(text) for text in texts_resampled]\n",
    "\n",
    "# Extract input_ids, attention_mask, and labels\n",
    "input_ids = np.array([x['input_ids'][0] for x in tokenized_datasets])\n",
    "attention_mask = np.array([x['attention_mask'][0] for x in tokenized_datasets])\n",
    "labels_resampled = to_categorical(y_resampled, num_classes=8)\n",
    "\n",
    "# Split the resampled data into training and evaluation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_ids, labels_resampled, test_size=0.2, random_state=42)\n",
    "attention_mask_train, attention_mask_val = train_test_split(attention_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_train, 'attention_mask': attention_mask_train}, y_train))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': X_val, 'attention_mask': attention_mask_val}, y_val))\n",
    "eval_dataset = eval_dataset.batch(16)\n",
    "\n",
    "# Initialize model\n",
    "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=8)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training loop using tf.GradientTape\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_mask, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=True)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch {epoch+1}/{3}')\n",
    "    for batch in train_dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        loss = train_step(input_ids, attention_mask, labels)\n",
    "        print(f'Loss: {loss.numpy()}')\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(dataset):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    for batch in dataset:\n",
    "        input_ids = batch[0]['input_ids']\n",
    "        attention_mask = batch[0]['attention_mask']\n",
    "        labels = batch[1]\n",
    "        logits = model(input_ids, attention_mask=attention_mask, training=False)[0]\n",
    "        loss = loss_fn(labels, logits)\n",
    "        total_loss += loss.numpy()\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(labels, axis=-1)), tf.float32))\n",
    "        total_accuracy += accuracy.numpy()\n",
    "        num_batches += 1\n",
    "    return total_loss / num_batches, total_accuracy / num_batches\n",
    "\n",
    "eval_loss, eval_accuracy = evaluate(eval_dataset)\n",
    "print(f'Evaluation Loss: {eval_loss}')\n",
    "print(f'Evaluation Accuracy: {eval_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
